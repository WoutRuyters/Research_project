{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation, BatchNormalization,SpatialDropout1D,Bidirectional, Embedding, LSTM\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow.contrib.layers as layers\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "pd.set_option('display.max_rows',1000)\n",
        "pd.set_option('display.max_columns',1000)\n",
        "\n",
        "\n",
        "# # # Voor GPU support\n",
        "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# Inlezen van de dataset\n",
        "\n",
        "dataset = pd.read_csv('./Dataset/mushrooms.csv')\n",
        "\n",
        "# Print de eerste 5 lijnen van de dataset\n",
        "dataset.head()\n",
        "\n",
        "# Controleren op null values\n",
        "dataset.isnull().sum()\n",
        "\n",
        "# Momenteel bestaat de data uit verschillende letters per kolom wat moeilijk te begrijpen is\n",
        "# Met de get_dummies function zorgen we ervoor dat we onze kolommen one-hot encoden\n",
        "data_dum = pd.get_dummies(dataset)\n",
        "\n",
        "# Print de shape van de one-hot encoded data\n",
        "data_dum.shape\n",
        "\n",
        "# Print de eerste 5 lijnen van de one-hot encoded data\n",
        "# Hierin kunnen we zien dat we nu veel meer kolommen hebben en de letters weg zijn\n",
        "data_dum.head()\n",
        "\n",
        "# Controleren of de dataset gebalanceerd is\n",
        "sns.countplot(x=\"class\", data=dataset)\n",
        "\n",
        "# Defini\u00eber X en Y\n",
        "\n",
        "# In onze x_data willen we alle data behalve of de paddenstoel eetbaar is of niet dus nemen we alle kolommen na deze 2\n",
        "# We zetten dit ook om naar een numpy-array met type float\n",
        "X_data = data_dum.loc[:, 'cap-shape_b':].to_numpy().astype(np.float32)\n",
        "\n",
        "\n",
        "# In onze y_data mogen we enkel de klasses p en e (poisonous & edible) hebben\n",
        "# We zetten dit ook om naar een numpy-array met type float\n",
        "y_data = data_dum.loc[:, :'class_p'].to_numpy().astype(np.float32)\n",
        "\n",
        "# We defini\u00ebren dat de trainingset 5000 samples moet zijn\n",
        "n = 5000\n",
        "\n",
        "# Neem de eerste 5000 samples van de x_data en steek deze in X_train\n",
        "X_train = X_data[:n]\n",
        "\n",
        "# Onze testset X_test krijgt de overige ~3000 samples\n",
        "X_test = X_data[n:]\n",
        "\n",
        "# Neem de eerste 5000 samples van de y_data en steek deze in y_train\n",
        "y_train = y_data[:n]\n",
        "\n",
        "# Onze testset y_test krijgt de overige ~3000 samples\n",
        "y_test = y_data[n:]\n",
        "\n",
        "# Print de shapes\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# Defini\u00eber de INPUT_SIZE met het aantal kolommen dat X_train heeft\n",
        "INPUT_SIZE = X_train.shape[1]\n",
        "\n",
        "# Defini\u00eber de OUTPUT_SIZE met het aantal kolommen dat y_train heeft\n",
        "OUTPUT_SIZE = y_train.shape[1]\n",
        "\n",
        "# Defini\u00eber EPOCHS_NUM, het aantal epochs dat het model moet doorlopen\n",
        "EPOCH_NUM = 50\n",
        "\n",
        "# Defini\u00eber de BATCH_SIZE dat het model moet gebruiken\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Maak een placeholder aan met het datatype en de shape voor de inputs\n",
        "inputs = tf.placeholder(tf.float32, shape=[None, INPUT_SIZE])\n",
        "\n",
        "# Maak een placeholder aan met het datatype en de shape voor de outputs\n",
        "outputs = tf.placeholder(tf.float32, shape=[None, OUTPUT_SIZE])\n",
        "\n",
        "# Maak een nieuwe variabele waarin we een truncated value maken om saturatie te voorkomen waardoor de neuroon niet meer zou bijleren\n",
        "w = tf.Variable(tf.truncated_normal([INPUT_SIZE, OUTPUT_SIZE], stddev=0.1), dtype=tf.float32)\n",
        "\n",
        "# Maak een nieuwe variabele waarin we een constante tensor aanmaken\n",
        "b = tf.Variable(tf.constant(0.1, shape=[OUTPUT_SIZE]), dtype=tf.float32)\n",
        "\n",
        "# Bereken de y_pred\n",
        "y_pred = tf.matmul(inputs, w) + b\n",
        "\n",
        "# Maak de de loss function\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=outputs, logits=y_pred))\n",
        "\n",
        "# Laat de Adam Optimizer de train step aanpassen\n",
        "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
        "\n",
        "# Defini\u00eber hoe de juiste prediction gevonden wordt\n",
        "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(outputs, 1))\n",
        "\n",
        "# Defini\u00eber hoe de accuracy berekend wordt\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# Maak een nieuwe Session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Maak een saver om het model op te slaan\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Run de session met de globale variabelen\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Blijf de for-loop uitvoeren tot het meegegeven aantal epochs is uitgevoerd\n",
        "for epoch in tqdm(range(EPOCH_NUM), file=sys.stdout):\n",
        "    # Maak een willekeurige permutatie in onze range van 5000 (n)\n",
        "    perm = np.random.permutation(n)\n",
        "\n",
        "    prev_test_acc = 0\n",
        "\n",
        "    for i in range(0, n, BATCH_SIZE):\n",
        "        # Defini\u00eber de X_batch via de eerder gekozen permutatie\n",
        "        X_batch = X_train[perm[i:i+BATCH_SIZE]]\n",
        "\n",
        "        # Defini\u00eber de y_batch via de eerder gekozen permutatie\n",
        "        y_batch = y_train[perm[i:i+BATCH_SIZE]]\n",
        "\n",
        "        # Run de train (adam)\n",
        "        train_step.run(session=sess, feed_dict={inputs: X_batch, outputs: y_batch})\n",
        "    \n",
        "    # Evalueer de accuracy op de training data\n",
        "    acc = accuracy.eval(session=sess, feed_dict={inputs: X_train, outputs: y_train})\n",
        "\n",
        "    # Evalueer de accuracy op de test data\n",
        "    test_acc = accuracy.eval(session=sess, feed_dict={inputs: X_test, outputs: y_test})\n",
        "\n",
        "    # Print de accuracy en validation accuracy elke epoch\n",
        "    if (epoch+1) % 1 == 0:\n",
        "        tqdm.write('epoch:\\t%i\\taccuracy:\\t%f\\tvalidation accuracy:\\t%f' % (epoch+1, acc, test_acc))\n",
        "\n",
        "        # Defini\u00eber het path naar waar het model gesaved moet worden en save het\n",
        "        if (test_acc > prev_test_acc):\n",
        "            save_path = saver.save(sess, './Models/model.ckpt')\n",
        "            print(\"Model saved in path: %s\" % save_path)\n",
        "            prev_test_acc = test_acc\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=5000, random_state=0)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(117,)),\n",
        "    tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
        "\ttf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(2, activation=tf.nn.sigmoid),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# pas earlystopping toe\n",
        "earlystopper = EarlyStopping(patience=20, verbose=1)\n",
        "\n",
        "# sla het beste model op\n",
        "checkpointer = ModelCheckpoint('./Models/model_keras.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "# train het model\n",
        "history = model.fit(X_train, y_train, epochs=10, verbose=1, batch_size=256, validation_data=(X_test, y_test), callbacks=[earlystopper, checkpointer])\n",
        "\n",
        "# Plot of the training history\n",
        "\n",
        "# Accuracy\n",
        "plt.plot(history.history['acc'],'r')\n",
        "plt.plot(history.history['val_acc'],'b')\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# Loss \n",
        "plt.plot(history.history['loss'],'r')\n",
        "plt.plot(history.history['val_loss'],'b')\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "rounded_labels=np.argmax(y_test, axis=1)\n",
        "\n",
        "# Testen met de test set\n",
        "# model = tf.keras.models.load_model('./Models/model_keras.h5')\n",
        "\n",
        "y_pred = model.predict_classes(X_test)\n",
        "print('\\n')\n",
        "print(classification_report(rounded_labels, y_pred))\n",
        "print('\\n')\n",
        "cf = confusion_matrix(rounded_labels, y_pred)\n",
        "print(accuracy_score(rounded_labels, y_pred) * 100)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print('MAE: %f' % (mean_absolute_error(y_pred, y_test)))\n",
        "print('R2: %f' % (r2_score(y_pred, y_test))) \n",
        "\n",
        "test_array_poison = np.array([[1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0]])\n",
        "\n",
        "test_array_edible = np.array([[0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0]])\n",
        "\n",
        "print(test_array_poison.shape)\n",
        "print(test_array_edible.shape)\n",
        "\n",
        "\n",
        "model.predict(test_array_poison)\n",
        "\n",
        "model.predict(test_array_edible)\n",
        "\n",
        "model.predict(test_array_edible)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}