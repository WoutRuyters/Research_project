{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-c3c872f69eeb>:110: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-c3c872f69eeb>:124: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From d:\\Wout\\School\\Semester 5\\Research Project\\Uitwerking\\Tensorflow_1\\.venv_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From d:\\Wout\\School\\Semester 5\\Research Project\\Uitwerking\\Tensorflow_1\\.venv_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From d:\\Wout\\School\\Semester 5\\Research Project\\Uitwerking\\Tensorflow_1\\.venv_tf1\\lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers\\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "(10, 100, 2)\n",
      "(10, 100, 1)\n",
      "WARNING:tensorflow:From d:\\Wout\\School\\Semester 5\\Research Project\\Uitwerking\\Tensorflow_1\\.venv_tf1\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "(10, 16, 2)\n",
      "(10, 16, 1)\n",
      "Epoch 0, train error: 0.69, valid accuracy: 55.1 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Short and sweet LSTM implementation in Tensorflow.\n",
    "Motivation:\n",
    "When Tensorflow was released, adding RNNs was a bit of a hack - it required\n",
    "building separate graphs for every number of timesteps and was a bit obscure\n",
    "to use. Since then TF devs added things like `dynamic_rnn`, `scan` and `map_fn`.\n",
    "Currently the APIs are decent, but all the tutorials that I am aware of are not\n",
    "making the best use of the new APIs.\n",
    "Advantages of this implementation:\n",
    "- No need to specify number of timesteps ahead of time. Number of timesteps is\n",
    "  infered from shape of input tensor. Can use the same graph for multiple\n",
    "  different numbers of timesteps.\n",
    "- No need to specify batch size ahead of time. Batch size is infered from shape\n",
    "  of input tensor. Can use the same graph for multiple different batch sizes.\n",
    "- Easy to swap out different recurrent gadgets (RNN, LSTM, GRU, your new\n",
    "  creative idea)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "map_fn = tf.map_fn\n",
    "\n",
    "################################################################################\n",
    "##                           DATASET GENERATION                               ##\n",
    "##                                                                            ##\n",
    "##  The problem we are trying to solve is adding two binary numbers. The      ##\n",
    "##  numbers are reversed, so that the state of RNN can add the numbers        ##\n",
    "##  perfectly provided it can learn to store carry in the state. Timestep t   ##\n",
    "##  corresponds to bit len(number) - t.                                       ##\n",
    "################################################################################\n",
    "\n",
    "def as_bytes(num, final_size):\n",
    "    res = []\n",
    "    for _ in range(final_size):\n",
    "        res.append(num % 2)\n",
    "        num //= 2\n",
    "    return res\n",
    "\n",
    "def generate_example(num_bits):\n",
    "    a = random.randint(0, 2**(num_bits - 1) - 1)\n",
    "    b = random.randint(0, 2**(num_bits - 1) - 1)\n",
    "    res = a + b\n",
    "    return (as_bytes(a,  num_bits),\n",
    "            as_bytes(b,  num_bits),\n",
    "            as_bytes(res,num_bits))\n",
    "\n",
    "def generate_batch(num_bits, batch_size):\n",
    "    \"\"\"Generates instance of a problem.\n",
    "    Returns\n",
    "    -------\n",
    "    x: np.array\n",
    "        two numbers to be added represented by bits.\n",
    "        shape: b, i, n\n",
    "        where:\n",
    "            b is bit index from the end\n",
    "            i is example idx in batch\n",
    "            n is one of [0,1] depending for first and\n",
    "                second summand respectively\n",
    "    y: np.array\n",
    "        the result of the addition\n",
    "        shape: b, i, n\n",
    "        where:\n",
    "            b is bit index from the end\n",
    "            i is example idx in batch\n",
    "            n is always 0\n",
    "    \"\"\"\n",
    "    x = np.empty((num_bits, batch_size, 2))\n",
    "    y = np.empty((num_bits, batch_size, 1))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        a, b, r = generate_example(num_bits)\n",
    "        x[:, i, 0] = a\n",
    "        x[:, i, 1] = b\n",
    "        y[:, i, 0] = r\n",
    "    return x, y\n",
    "\n",
    "\n",
    "################################################################################\n",
    "##                           GRAPH DEFINITION                                 ##\n",
    "################################################################################\n",
    "\n",
    "INPUT_SIZE    = 2       # 2 bits per timestep\n",
    "RNN_HIDDEN    = 20\n",
    "OUTPUT_SIZE   = 1       # 1 bit per timestep\n",
    "TINY          = 1e-6    # to avoid NaNs in logs\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "USE_LSTM = True\n",
    "\n",
    "inputs  = tf.placeholder(tf.float32, (None, None, INPUT_SIZE))  # (time, batch, in)\n",
    "outputs = tf.placeholder(tf.float32, (None, None, OUTPUT_SIZE)) # (time, batch, out)\n",
    "\n",
    "\n",
    "## Here cell can be any function you want, provided it has two attributes:\n",
    "#     - cell.zero_state(batch_size, dtype)- tensor which is an initial value\n",
    "#                                           for state in __call__\n",
    "#     - cell.__call__(input, state) - function that given input and previous\n",
    "#                                     state returns tuple (output, state) where\n",
    "#                                     state is the state passed to the next\n",
    "#                                     timestep and output is the tensor used\n",
    "#                                     for infering the output at timestep. For\n",
    "#                                     example for LSTM, output is just hidden,\n",
    "#                                     but state is memory + hidden\n",
    "# Example LSTM cell with learnable zero_state can be found here:\n",
    "#    https://gist.github.com/nivwusquorum/160d5cf7e1e82c21fad3ebf04f039317\n",
    "if USE_LSTM:\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(RNN_HIDDEN, state_is_tuple=True)\n",
    "else:\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(RNN_HIDDEN)\n",
    "\n",
    "# Create initial state. Here it is just a constant tensor filled with zeros,\n",
    "# but in principle it could be a learnable parameter. This is a bit tricky\n",
    "# to do for LSTM's tuple state, but can be achieved by creating two vector\n",
    "# Variables, which are then tiled along batch dimension and grouped into tuple.\n",
    "batch_size    = tf.shape(inputs)[1]\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# Given inputs (time, batch, input_size) outputs a tuple\n",
    "#  - outputs: (time, batch, output_size)  [do not mistake with OUTPUT_SIZE]\n",
    "#  - states:  (time, batch, hidden_size)\n",
    "rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, time_major=True)\n",
    "\n",
    "# project output from rnn output size to OUTPUT_SIZE. Sometimes it is worth adding\n",
    "# an extra layer here.\n",
    "final_projection = lambda x: layers.linear(x, num_outputs=OUTPUT_SIZE, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "# apply projection to every timestep.\n",
    "predicted_outputs = map_fn(final_projection, rnn_outputs)\n",
    "\n",
    "# compute elementwise cross entropy.\n",
    "error = -(outputs * tf.log(predicted_outputs + TINY) + (1.0 - outputs) * tf.log(1.0 - predicted_outputs + TINY))\n",
    "error = tf.reduce_mean(error)\n",
    "\n",
    "# optimize\n",
    "train_fn = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(error)\n",
    "\n",
    "# assuming that absolute difference between output and correct answer is 0.5\n",
    "# or less we can round it to the correct output.\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.abs(outputs - predicted_outputs) < 0.5, tf.float32))\n",
    "\n",
    "\n",
    "################################################################################\n",
    "##                           TRAINING LOOP                                    ##\n",
    "################################################################################\n",
    "\n",
    "NUM_BITS = 10\n",
    "ITERATIONS_PER_EPOCH = 100\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "valid_x, valid_y = generate_batch(num_bits=NUM_BITS, batch_size=100)\n",
    "\n",
    "print(valid_x.shape)\n",
    "print(valid_y.shape)\n",
    "\n",
    "session = tf.Session()\n",
    "# For some reason it is our job to do this:\n",
    "session.run(tf.initialize_all_variables())\n",
    "\n",
    "for epoch in range(1):\n",
    "    epoch_error = 0\n",
    "    for _ in range(ITERATIONS_PER_EPOCH):\n",
    "        # here train_fn is what triggers backprop. error and accuracy on their\n",
    "        # own do not trigger the backprop.\n",
    "        x, y = generate_batch(num_bits=NUM_BITS, batch_size=BATCH_SIZE)\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        epoch_error += session.run([error, train_fn], {\n",
    "            inputs: x,\n",
    "            outputs: y,\n",
    "        })[0]\n",
    "    epoch_error /= ITERATIONS_PER_EPOCH\n",
    "    valid_accuracy = session.run(accuracy, {\n",
    "        inputs:  valid_x,\n",
    "        outputs: valid_y,\n",
    "    })\n",
    "    print(\"Epoch %d, train error: %.2f, valid accuracy: %.1f %%\" % (epoch, epoch_error, valid_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}